# par(mfrow=c(1,1))
# Green do have a higher mean and median
greenbuildings %>% group_by(green_rating) %>% summarise(avg_rent = mean(Rent),
median_rent = median(Rent))
"Does the scrubbing make sense?:
I'm not really sure. I lean towards yes because I think it is probably a data entry error, but a counterpoint
is the other values associated with this points seems to be valid still.
It doesn't really affect results much so I guess removing should be fine? Just make careful note of it.
Maybe only delete the ones that are 0?
"
# How many would that remove?
sum(greenbuildings$leasing_rate < 10) # 215. Not terrible
# Clean it
scrubbed_buildings = greenbuildings[greenbuildings$leasing_rate >= 10,]
# Same median as before? That seems weird right?
scrubbed_buildings %>% group_by(green_rating) %>% summarise(avg_rent = mean(Rent),
median_rent = median(Rent))
# Using median seems fair. Especially when considering the healthy chunk of data points beyond the upper fence
ggplot(scrubbed_buildings, mapping = aes(y=Rent)) + geom_boxplot()
ggplot(scrubbed_buildings, mapping = aes(x=Rent)) + geom_histogram() # Again, slight left skew
# hard to see any difference here
ggplot(scrubbed_buildings, mapping = aes(x=as.factor(green_rating),y=Rent)) + geom_boxplot()
ggplot(scrubbed_buildings, mapping = aes(x=as.factor(renovated),y=Rent)) + geom_boxplot()
ggplot(scrubbed_buildings, mapping = aes(x=as.factor(amenities),y=Rent)) + geom_boxplot() + facet_wrap(~renovated)
# Maybe upward trend in rent for size?
ggplot(scrubbed_buildings, mapping = aes(log(size),y=Rent)) + geom_point()
greenbuildings = greenbuildings[,-1]
lm_model = lm(Rent~.,greenbuildings)
summary(lm_model)
library(readr)
library(dplyr)
library(tidyverse)
greenbuildings <- read_csv("greenbuildings.csv")
# par(mfrow=c(1,1))
# Green do have a higher mean and median
greenbuildings %>% group_by(green_rating) %>% summarise(avg_rent = mean(Rent),
median_rent = median(Rent))
"Does the scrubbing make sense?:
I'm not really sure. I lean towards yes because I think it is probably a data entry error, but a counterpoint
is the other values associated with this points seems to be valid still.
It doesn't really affect results much so I guess removing should be fine? Just make careful note of it.
Maybe only delete the ones that are 0?
"
# How many would that remove?
sum(greenbuildings$leasing_rate < 10) # 215. Not terrible
# Clean it
scrubbed_buildings = greenbuildings[greenbuildings$leasing_rate >= 10,]
# Same median as before? That seems weird right?
scrubbed_buildings %>% group_by(green_rating) %>% summarise(avg_rent = mean(Rent),
median_rent = median(Rent))
# Using median seems fair. Especially when considering the healthy chunk of data points beyond the upper fence
ggplot(scrubbed_buildings, mapping = aes(y=Rent)) + geom_boxplot()
ggplot(scrubbed_buildings, mapping = aes(x=Rent)) + geom_histogram() # Again, slight left skew
# hard to see any difference here
ggplot(scrubbed_buildings, mapping = aes(x=as.factor(green_rating),y=Rent)) + geom_boxplot()
ggplot(scrubbed_buildings, mapping = aes(x=as.factor(renovated),y=Rent)) + geom_boxplot()
ggplot(scrubbed_buildings, mapping = aes(x=as.factor(amenities),y=Rent)) + geom_boxplot() + facet_wrap(~renovated)
# Maybe upward trend in rent for size?
ggplot(scrubbed_buildings, mapping = aes(log(size),y=Rent)) + geom_point()
greenbuildings = greenbuildings[,-c(1,2)]
# USe a regression to figure out whats important?
lm_model = lm(Rent~.,greenbuildings)
summary(lm_model)
scrubbed_buildings %>% group_by(green_rating) %>% summarise(cluster_rent)
scrubbed_buildings %>% group_by(green_rating) %>% summarise(avg_cluster_rent = mean(cluster_rent)
scrubbed_buildings %>% group_by(green_rating) %>% summarise(avg_cluster_rent = mean(cluster_rent))
scrubbed_buildings %>% group_by(green_rating) %>% summarise(avg_cluster_rent = mean(cluster_rent))
greenbuildings %>% group_by(green_rating) %>% summarise(avg_cluster_rent = mean(cluster_rent))
scrubbed_buildings
scrubbed_buildings %>% group_by(green_rating) %>% summarise(avg_cluster_rent = mean(cluster_rent))
ggplot(cluster_rent) + geom_bar(aes(green_rating,avg_cluster_rent))
cluster_rent = scrubbed_buildings %>% group_by(green_rating) %>% summarise(avg_cluster_rent = mean(cluster_rent))
ggplot(cluster_rent) + geom_bar(aes(green_rating,avg_cluster_rent))
ggplot(cluster_rent) + geom_col(aes(green_rating,avg_cluster_rent))
ggplot(cluster_rent) + geom_col(aes(green_rating,avg_cluster_rent,color=green_rating))
?geom_bar
ggplot(cluster_rent) + geom_col(aes(green_rating,avg_cluster_rent,color=green_rating))
ggplot(cluster_rent) + geom_col(aes(green_rating,avg_cluster_rent,fill=green_rating))
ggplot(cluster_rent) + geom_col(aes(as.factor(green_rating),avg_cluster_rent,fill=green_rating))
summary(lm_model)
scrubbed_buildings %>% group_by(green_rating) %>%
summarise(avg_cluster_rent = mean(cluster_rent)) %>% ggplot() + geom_col(aes(as.factor(green_rating),avg_cluster_rent,fill=green_rating))
scrubbed_buildings %>% group_by(green_rating) %>%
summarise(avg_cluster_rent = mean(cluster_rent)) %>% ggplot() + geom_col(aes(as.factor(green_rating),avg_cluster_rent,fill=green_rating))
sum(greenbuildings$net)
# Machine-Learning
library(tidyverse)
library(dplyr)
library(mosaic)
library(quantmod)
library(foreach)
# Get ETs
my_etfs = c("ARKK", "SDY", "GOVT",'BNO','FXY','XLF','MCHI','HYG')
etf_count = length(my_etfs)
# 5 years of data
getSymbols(my_etfs,from='2016-8-1')
# We want to look at adjusted values only
for(ticker in my_etfs) {
expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
eval(parse(text=expr))
}
my_etfs_adjusted = paste(my_etfs,'a',sep = '')
all_returns = cbind( ClCl(ARKKa),
ClCl(SDYa),
ClCl(GOVTa),
ClCl(BNOa),
ClCl(FXYa),
ClCl(XLFa),
ClCl(MCHIa),
ClCl(HYGa))
all_returns = as.matrix(na.omit(all_returns))
head(all_returns)
allocation_equal = rep(1/etf_count,etf_count)
# Create bond allocation. 60% in two bond funds. Equal weight placed in the remaining
bond_allocation = .6
non_bond = (1-bond_allocation)/6
allocation_bond_heavy = rep(non_bond,etf_count)
allocation_bond_heavy[c(3,4)] = bond_allocation/2
# Foreign Heavy
foreign_allocation = .6
non_foreign = (1-foreign_allocation)/5
allocation_foreign = rep(non_foreign,etf_count)
allocation_foreign[c(4,5,6)] = foreign_allocation/3
# Compile three portfolios in a matrix
portfolios = cbind(allocation_equal,allocation_bond_heavy,allocation_foreign)
initial_capital = 100000
days_in_simulaiton = 20
simulation_count = 1000
all_ending_wealths = rep(0,simulation_count)
for (simulation_num in 1:simulation_count) {
wealth = initial_capital
for (day_num in 1:days_in_simulaiton) {
returns = resample(all_returns,1)
wealth = wealth + wealth * returns
}
all_ending_wealths[simulation_num] = wealth
}
hist(all_ending_wealths)
# Sample the returns
resample(all_returns, 1, orig.ids=FALSE)
View(portfolios)
# Make sure each of the allocations sum to one
sum(portfolios)
?sum
portfolios %>% summarise_all(mean)
rowsum(portfolios)
colSums(portfolios)
# Equal weighted
allocation_equal = rep(1/etf_count,etf_count)
# Create bond allocation. 60% in two bond funds. Equal weight placed in the remaining
bond_allocation = .8
non_bond = (1-bond_allocation)/6
allocation_bond_heavy = rep(non_bond,etf_count)
allocation_bond_heavy[c(3,4)] = bond_allocation/2
# Foreign Heavy
foreign_allocation = .8
non_foreign = (1-foreign_allocation)/5
allocation_foreign = rep(non_foreign,etf_count)
allocation_foreign[c(4,5,6)] = foreign_allocation/3
# Compile three portfolios in a matrix
portfolios = cbind(allocation_equal,allocation_bond_heavy,allocation_foreign)
# Make sure each of the allocations sum to one
colSums(portfolios)
# Make sure each of the allocations sum to one
colSums(portfolios) == 1
source("C:/Users/User/OneDrive/Desktop/STA380/R/portfolio.R", echo=TRUE)
View(sim1)
as.data.fram(cbind(allocation_equal,allocation_bond_heavy,allocation_foreign))
as.data.frame(cbind(allocation_equal,allocation_bond_heavy,allocation_foreign))
x = as.data.frame(cbind(allocation_equal,allocation_bond_heavy,allocation_foreign))
x = as.data.frame(allocation_equal,allocation_bond_heavy,allocation_foreign)
x = data.frame()
x
?rep
returns = data.matrix()
?data.matrix
vector()
append(vector(),5)
x = append(vector(),5)
x = append(x,5)
x
x = append(vector(),c(5,5))
x
x = append(x,5)
x
# Compile three portfolios in a matrix
portfolio_weights = as.data.frame(cbind(allocation_equal,allocation_bond_heavy,allocation_foreign))
rep(0,days_in_simulaiton* simulation_count)
?data.matrix
equal_weight_wealths = data.matrix(rep(0,days_in_simulaiton* simulation_count),nrow=simulation_count,ncol=days_in_simulaiton)
equal_weight_wealths = matrix(rep(0,days_in_simulaiton* simulation_count),nrow=simulation_count,ncol=days_in_simulaiton)
View(equal_weight_wealths)
source("C:/Users/User/OneDrive/Desktop/Machine Learning/Machine-Learning/Portfolio.R", echo=TRUE)
# Make sure each of the allocations sum to one
colSums(portfolio_weights) == 1
# Simulation Parameters
initial_capital = 100000
days_in_simulaiton = 20
simulation_count = 1000
all_ending_wealths = rep(0,simulation_count)
equal_weight_wealths = matrix(rep(0,days_in_simulaiton* simulation_count),nrow=simulation_count,ncol=days_in_simulaiton)
bondy_heavy_wealths = matrix(rep(0,days_in_simulaiton* simulation_count),nrow=simulation_count,ncol=days_in_simulaiton)
foreign_heavy_wealths = matrix(rep(0,days_in_simulaiton* simulation_count),nrow=simulation_count,ncol=days_in_simulaiton)
for (simulation_num in 1:simulation_count) {
equal_weight_port = portfolio_weights$allocation_equal * initial_capital
bond_heavy_port = portfolio_weights$allocation_bond_heavy * initial_capital
foreign_heavy_port = portfolio_weights$allocation_foreign * initial_capital
for (day_num in 1:days_in_simulaiton) {
returns = resample(all_returns,1) # Sample returns
# Calculate ending wealth after each day
equal_weight_port = equal_weight_port + equal_weight_port * returns
bond_heavy_port = bond_heavy_port + bond_heavy_port * returns
foreign_heavy_port = foreign_heavy_port + foreign_heavy_port * returns
# Record the wealths for each portfolio at that point in time
equal_weight_wealths[simulation_num,day_num] = sum(equal_weight_port)
bondy_heavy_wealths[simulation_num,day_num] = sum(bond_heavy_port)
foreign_heavy_wealths[simulation_num,day_num] = sum(foreign_heavy_port)
# Re balance each portfolio
equal_weight_port = equal_weight_port * portfolio_weights$allocation_equal
bond_heavy_port = bond_heavy_port + portfolio_weights$allocation_bond_heavy
foreign_heavy_port = foreign_heavy_port + portfolio_weights$allocation_foreign
}
}
hist(equal_weight_wealths[,days_in_simulaiton])
View(portfolio_weights)
ggplot()
equal_weight_wealths
x = as.data.frame(equal_weight_wealths)
ggplot(x) + geom_histogram(aes(V20))
View(bond_heavy_port)
sum(bond_heavy_port)
View(foreign_heavy_wealths)
View(portfolio_weights)
hist(equal_weight_wealths[,days_in_simulaiton])
hist(equal_weight_wealths[,days_in_simulaiton],bins=40)
?hist
hist(equal_weight_wealths[,days_in_simulaiton],breaks=40)
View(equal_weight_wealths)
all_ending_wealths = rep(0,simulation_count)
equal_weight_wealths = matrix(rep(0,days_in_simulaiton* simulation_count),nrow=simulation_count,ncol=days_in_simulaiton)
bondy_heavy_wealths = matrix(rep(0,days_in_simulaiton* simulation_count),nrow=simulation_count,ncol=days_in_simulaiton)
foreign_heavy_wealths = matrix(rep(0,days_in_simulaiton* simulation_count),nrow=simulation_count,ncol=days_in_simulaiton)
for (simulation_num in 1:simulation_count) {
equal_weight_port = portfolio_weights$allocation_equal * initial_capital
bond_heavy_port = portfolio_weights$allocation_bond_heavy * initial_capital
foreign_heavy_port = portfolio_weights$allocation_foreign * initial_capital
for (day_num in 1:days_in_simulaiton) {
returns = resample(all_returns,1) # Sample returns
# Calculate ending wealth after each day
equal_weight_port = equal_weight_port + equal_weight_port * returns
bond_heavy_port = bond_heavy_port + bond_heavy_port * returns
foreign_heavy_port = foreign_heavy_port + foreign_heavy_port * returns
# Record the wealths for each portfolio at that point in time
equal_weight_wealths[simulation_num,day_num] = sum(equal_weight_port)
bondy_heavy_wealths[simulation_num,day_num] = sum(bond_heavy_port)
foreign_heavy_wealths[simulation_num,day_num] = sum(foreign_heavy_port)
# Re balance each portfolio
equal_weight_port = equal_weight_port + portfolio_weights$allocation_equal
bond_heavy_port = bond_heavy_port + portfolio_weights$allocation_bond_heavy
foreign_heavy_port = foreign_heavy_port + portfolio_weights$allocation_foreign
}
}
hist(equal_weight_wealths[,days_in_simulaiton],breaks=40)
?abline
abline(v=100000)
abline(v=100000,lwd=3,col='blue')
portfolio_weights = as.data.frame(cbind(allocation_equal,allocation_bond_heavy,allocation_foreign))
# Make sure each of the allocations sum to one
colSums(portfolio_weights) == 1
# Simulation Parameters
initial_capital = 100000
days_in_simulation = 20
simulation_count = 1000
all_ending_wealths = rep(0,simulation_count)
equal_weight_wealths = matrix(rep(0,days_in_simulation* simulation_count),nrow=simulation_count,ncol=days_in_simulation)
bondy_heavy_wealths = matrix(rep(0,days_in_simulation* simulation_count),nrow=simulation_count,ncol=days_in_simulation)
foreign_heavy_wealths = matrix(rep(0,days_in_simulation* simulation_count),nrow=simulation_count,ncol=days_in_simulation)
for (simulation_num in 1:simulation_count) {
equal_weight_port = portfolio_weights$allocation_equal * initial_capital
bond_heavy_port = portfolio_weights$allocation_bond_heavy * initial_capital
foreign_heavy_port = portfolio_weights$allocation_foreign * initial_capital
for (day_num in 1:days_in_simulation) {
returns = resample(all_returns,1) # Sample returns
# Calculate ending wealth after each day
equal_weight_port = equal_weight_port + equal_weight_port * returns
bond_heavy_port = bond_heavy_port + bond_heavy_port * returns
foreign_heavy_port = foreign_heavy_port + foreign_heavy_port * returns
# Record the wealths for each portfolio at that point in time
equal_weight_wealths[simulation_num,day_num] = sum(equal_weight_port)
bondy_heavy_wealths[simulation_num,day_num] = sum(bond_heavy_port)
foreign_heavy_wealths[simulation_num,day_num] = sum(foreign_heavy_port)
# Re balance each portfolio
equal_weight_port = equal_weight_port + portfolio_weights$allocation_equal
bond_heavy_port = bond_heavy_port + portfolio_weights$allocation_bond_heavy
foreign_heavy_port = foreign_heavy_port + portfolio_weights$allocation_foreign
}
}
hist(equal_weight_wealths[,days_in_simulation],breaks=40)
abline(v=intial_capital,lwd=3,col='blue')
hist(equal_weight_wealths[,days_in_simulation],breaks=40)
abline(v=intial_capital,lwd=3,col='blue')
hist(equal_weight_wealths[,days_in_simulation],breaks=40)
abline(v=initial_capital,lwd=3,col='blue')
hist(equal_weight_wealths[,days_in_simulation],breaks=40)
abline(v=initial_capital,lwd=3,col='blue')
par(mfrow=c(2,2))
hist(equal_weight_wealths[,days_in_simulation],breaks=40)
abline(v=initial_capital,lwd=3,col='blue')
hist(bondy_heavy_wealths[,days_in_simulation],breaks=40)
abline(v=initial_capital,lwd=3,col='blue')
hist(foreign_heavy_wealths[,days_in_simulation],breaks=40)
abline(v=initial_capital,lwd=3,col='blue')
par(mfrow=c(1,3))
hist(equal_weight_wealths[,days_in_simulation],breaks=40)
abline(v=initial_capital,lwd=3,col='blue')
hist(bondy_heavy_wealths[,days_in_simulation],breaks=40)
abline(v=initial_capital,lwd=3,col='blue')
hist(foreign_heavy_wealths[,days_in_simulation],breaks=40)
abline(v=initial_capital,lwd=3,col='blue')
par(mfrow=c(3,1))
hist(equal_weight_wealths[,days_in_simulation],breaks=40)
abline(v=initial_capital,lwd=3,col='blue')
hist(bondy_heavy_wealths[,days_in_simulation],breaks=40)
abline(v=initial_capital,lwd=3,col='blue')
hist(foreign_heavy_wealths[,days_in_simulation],breaks=40)
abline(v=initial_capital,lwd=3,col='blue')
par(mfrow=c(2,2))
hist(equal_weight_wealths[,days_in_simulation],breaks=40)
abline(v=initial_capital,lwd=3,col='blue')
hist(bondy_heavy_wealths[,days_in_simulation],breaks=40)
abline(v=initial_capital,lwd=3,col='blue')
hist(foreign_heavy_wealths[,days_in_simulation],breaks=40)
abline(v=initial_capital,lwd=3,col='blue')
par(mfrow=c(2,2))
hist(equal_weight_wealths[,days_in_simulation],breaks=30)
abline(v=initial_capital,lwd=3,col='blue')
hist(bondy_heavy_wealths[,days_in_simulation],breaks=30)
abline(v=initial_capital,lwd=3,col='blue')
hist(foreign_heavy_wealths[,days_in_simulation],breaks=30)
abline(v=initial_capital,lwd=3,col='blue')
?quantile
?quantile(equal_weight_wealths[,20])
quantile(equal_weight_wealths[,20])
quantile(equal_weight_wealths[,20],.5)
quantile(equal_weight_wealths[,20],.05)
# 5% value at risk:
quantile(equal_weight_wealths[,days_in_simulation]- initial_wealth, prob=0.05)
# 5% value at risk:
quantile(equal_weight_wealths[,days_in_simulation]- initial_capital, prob=0.05)
quantile(equal_weight_wealths[,days_in_simulation]- initial_capital, prob=0.05)
quantile(bondy_heavy_wealths[,days_in_simulation]- initial_capital, prob=0.05)
quantile(foreign_heavy_wealths[,days_in_simulation]- initial_capital, prob=0.05)
View(all_returns)
hist(all_returns[,3])
hist(all_returns[,3],20)
greenbuildings <- read_csv("greenbuildings.csv")
View(greenbuildings)
View(greenbuildings)
library(readr)
library(dplyr)
library(tidyverse)
greenbuildings <- read_csv("greenbuildings.csv")
# par(mfrow=c(1,1))
# Green do have a higher mean and median
greenbuildings %>% group_by(green_rating) %>% summarise(avg_rent = mean(Rent),
median_rent = median(Rent))
"Does the scrubbing make sense?:
I'm not really sure. I lean towards yes because I think it is probably a data entry error, but a counterpoint
is the other values associated with this points seems to be valid still.
It doesn't really affect results much so I guess removing should be fine? Just make careful note of it.
Maybe only delete the ones that are 0?
"
# How many would that remove?
sum(greenbuildings$leasing_rate < 10) # 215. Not terrible
# Clean it
scrubbed_buildings = greenbuildings[greenbuildings$leasing_rate >= 10,]
# Same median as before? That seems weird right?
scrubbed_buildings %>% group_by(green_rating) %>% summarise(avg_rent = mean(Rent),
median_rent = median(Rent))
# Using median seems fair. Especially when considering the healthy chunk of data points beyond the upper fence
ggplot(scrubbed_buildings, mapping = aes(y=Rent)) + geom_boxplot()
ggplot(scrubbed_buildings, mapping = aes(x=Rent)) + geom_histogram() # Again, slight left skew
# hard to see any difference here
ggplot(scrubbed_buildings, mapping = aes(x=as.factor(green_rating),y=Rent)) + geom_boxplot()
ggplot(scrubbed_buildings, mapping = aes(x=as.factor(renovated),y=Rent)) + geom_boxplot()
ggplot(scrubbed_buildings, mapping = aes(x=as.factor(amenities),y=Rent)) + geom_boxplot() + facet_wrap(~renovated)
# Maybe upward trend in rent for size?
ggplot(scrubbed_buildings, mapping = aes(log(size),y=Rent)) + geom_point()
greenbuildings = greenbuildings[,-c(1,2)]
# Use a regression to figure out whats important?
# None of the coefficients are that inspiring.
lm_model = lm(Rent~.,greenbuildings)
summary(lm_model)
cor(greenbuildings$age,greenbuildings$Rent)
cor(greenbuildings$size,greenbuildings$Rent)
plot(greenbuildings$size,greenbuildings$Rent)
plot(greenbuildings$green_rating,greenbuildings$Rent)
par(mfrow=c(1,1))
plot(greenbuildings$green_rating,greenbuildings$Rent)
plot(greenbuildings$green_rating,greenbuildings$age)
box(greenbuildings$green_rating,greenbuildings$age)
boxplot(greenbuildings$green_rating,greenbuildings$age)
library(readr)
library(dplyr)
library(tidyverse)
greenbuildings <- read_csv("greenbuildings.csv")
# par(mfrow=c(1,1))
# Green do have a higher mean and median
greenbuildings %>% group_by(green_rating) %>% summarise(avg_rent = mean(Rent),
median_rent = median(Rent))
"Does the scrubbing make sense?:
I'm not really sure. I lean towards yes because I think it is probably a data entry error, but a counterpoint
is the other values associated with this points seems to be valid still.
It doesn't really affect results much so I guess removing should be fine? Just make careful note of it.
Maybe only delete the ones that are 0?
"
# How many would that remove?
sum(greenbuildings$leasing_rate < 10) # 215. Not terrible
# Clean it
scrubbed_buildings = greenbuildings[greenbuildings$leasing_rate >= 10,]
# Same median as before? That seems weird right?
scrubbed_buildings %>% group_by(green_rating) %>% summarise(avg_rent = mean(Rent),
median_rent = median(Rent))
# Using median seems fair. Especially when considering the healthy chunk of data points beyond the upper fence
ggplot(scrubbed_buildings, mapping = aes(y=Rent)) + geom_boxplot()
ggplot(scrubbed_buildings, mapping = aes(x=Rent)) + geom_histogram() # Again, slight left skew
# hard to see any difference here
ggplot(scrubbed_buildings, mapping = aes(x=as.factor(green_rating),y=Rent)) + geom_boxplot()
ggplot(scrubbed_buildings, mapping = aes(x=as.factor(amenities),y=Rent)) + geom_boxplot() + facet_wrap(~renovated)
ggplot(scrubbed_buildings, mapping = aes(x=as.factor(renovated),y=Rent)) + geom_boxplot()
# Maybe upward trend in rent for size?
ggplot(scrubbed_buildings, mapping = aes(log(size),y=Rent)) + geom_point()
ggplot(scrubbed_buildings, mapping = aes(x=as.factor(green_rating),y=size)) + geom_boxplot()
scrubbed_buildings %>% group_by(green_rating) %>% summarise(meany = mean(size))
ggplot(scrubbed_buildings, mapping = aes(x=Rent)) + geom_histogram()
ggplot(scrubbed_buildings, mapping = aes(x=size)) + geom_histogram() +
# Trying to show correlation between rating and size, then size and rent
ggplot(scrubbed_buildings, mapping = aes(x=as.factor(green_rating),y=size)) + geom_boxplot()
ggplot(scrubbed_buildings, mapping = aes(x=size)) + geom_histogram()
ggplot(scrubbed_buildings, mapping = aes(x=size, color=green_rating)) + geom_histogram()
ggplot(scrubbed_buildings, mapping = aes(x=size) + geom_histogram()  + facet_wrap(~green_rating)
ggplot(scrubbed_buildings, mapping = aes(x=size)) + geom_histogram()  + facet_wrap(~green_rating)
ggplot(scrubbed_buildings, mapping = aes(x=size)) + geom_histogram()  + facet_wrap(~green_rating)
ggplot(scrubbed_buildings, mapping = aes(x=size)) + geom_histogram(aes(y=stat(count/sum(count))))  + facet_wrap(~green_rating)
ggplot(scrubbed_buildings, mapping = aes(x=size)) + geom_histogram(aes(y=stat(count/sum(count))))
plot(greenbuildings$green_rating,greenbuildings$class_a)
jitter(1)
jitter(greenbuildings$green_rating)
plot(jitter(greenbuildings$green_rating),jitter(greenbuildings$class_a)
)
cor(greenbuildings$green_rating,greenbuildings$class_a)
corr(scrubbed_buildings)
cor(scrubbed_buildings)
greenbuildings = greenbuildings[,-c(1,2)] # 3 get rid of id and cluster num
cor(greenbuildings)
ggplot(cluster_rent) + geom_col(aes(as.factor(green_rating),avg_cluster_rent,fill=green_rating))
library(readr)
library(dplyr)
library(tidyverse)
greenbuildings <- read_csv("greenbuildings.csv")
# par(mfrow=c(1,1))
# Green do have a higher mean and median
greenbuildings %>% group_by(green_rating) %>% summarise(avg_rent = mean(Rent),
median_rent = median(Rent))
"Does the scrubbing make sense?:
I'm not really sure. I lean towards yes because I think it is probably a data entry error, but a counterpoint
is the other values associated with this points seems to be valid still.
It doesn't really affect results much so I guess removing should be fine? Just make careful note of it.
Maybe only delete the ones that are 0?
"
# How many would that remove?
sum(greenbuildings$leasing_rate < 10) # 215. Not terrible
# Clean it
scrubbed_buildings = greenbuildings[greenbuildings$leasing_rate >= 10,]
# Same median as before? That seems weird right?
scrubbed_buildings %>% group_by(green_rating) %>% summarise(avg_rent = mean(Rent),
median_rent = median(Rent))
# Using median seems fair. Especially when considering the healthy chunk of data points beyond the upper fence
ggplot(scrubbed_buildings, mapping = aes(y=Rent)) + geom_boxplot()
ggplot(scrubbed_buildings, mapping = aes(x=Rent)) + geom_histogram() # Again, slight left skew
# hard to see any difference here
ggplot(scrubbed_buildings, mapping = aes(x=as.factor(green_rating),y=Rent)) + geom_boxplot()
ggplot(scrubbed_buildings, mapping = aes(x=as.factor(renovated),y=Rent)) + geom_boxplot()
ggplot(scrubbed_buildings, mapping = aes(x=as.factor(amenities),y=Rent)) + geom_boxplot() + facet_wrap(~renovated)
ggplot(scrubbed_buildings, mapping = aes(x=size)) + geom_histogram()  + facet_wrap(~green_rating)
# Trying to show correlation between rating and size, then size and rent
ggplot(scrubbed_buildings, mapping = aes(x=as.factor(green_rating),y=size)) + geom_boxplot()
# Maybe upward trend in rent for size?
ggplot(scrubbed_buildings, mapping = aes(log(size),y=Rent)) + geom_point()
greenbuildings = greenbuildings[,-c(1,2)] # 3 get rid of id and cluster num
# Use a regression to figure out whats important?
# None of the coefficients are that inspiring.
lm_model = lm(Rent~.,greenbuildings)
summary(lm_model)
# Except cluster rent is highly significant and almost a 1 for 1 relationship.
# Maybe green buildings happen to be located in more expensive clusters?
cluster_rent = scrubbed_buildings %>% group_by(green_rating) %>% summarise(avg_cluster_rent = mean(cluster_rent))
# Not true, as this group by shows
ggplot(cluster_rent) + geom_col(aes(as.factor(green_rating),avg_cluster_rent,fill=green_rating))
install.packages("ggcorrplot")
library(ggcorrplot)
corrplot::corrplot(cor(greenbuildings))
cor(greenbuildings)
is.null(greenbuildings$empl_gr)
is.na(greenbuildings$empl_gr)
sum(is.na(greenbuildings$empl_gr))
no_nas = na.omi(greenbuildings)
no_nas = na.omit(greenbuildings)
corry = cor(no_nas)
corry
ggcorrplot(corry)
ggcorrplot(corry, hc.order = TRUE, outline.color = "white")
