all_ending_wealths %>%  ggplot(aes(x=X20)) + geom_histogram(fill='white', color='black', bins = 50) + facet_grid(Type ~.,scales = 'free') + geom_vline(xintercept = 100000, size=1,color='blue')+ scale_x_continuous(labels = scales::dollar_format()) + labs(title='Distribution of Ending Wealths By Portfolio')
# Var
all_ending_wealths$profit = all_ending_wealths$X20 - initial_capital
ggplot(all_ending_wealths,aes(profit)) + geom_histogram(fill='white', color='black', bins = 50) + facet_grid(Type ~.,scales = 'free') + scale_x_continuous(labels = scales::dollar_format()) + labs(title='Distribution of Ending Profits By Portfolio')
summary_stats = all_ending_wealths %>% group_by(Type) %>% summarise(VAR_.05 = quantile(profit,prob=.05), Minimum_Profit = min(profit),Max_Profit = max(profit), Average_Profit= mean(profit), MAD = mad(profit), Range = max(profit)-min(profit))
knitr::kable(summary_stats,caption = "Summary Stats for Portfolio Profits", digits = 2,format.args = list(big.mark = ",",
scientific = FALSE))
# Getting the wealth paths for each portfolio for the worst profit.
minports =  data.frame(all_ending_wealths %>%  group_by(Type) %>% filter(profit == min(profit)))
wealth_paths = data.frame(t(minports %>% select(-Type,-profit)))
colnames(wealth_paths) = c('Bond_Heavy', 'Equal_Weight', 'Foreign_Heavy')
wealth_paths$Day = 1:20
colors = c("Bond_Heavy" = "Green", "Equal_Weight" = "Blue", "Foreign_Heavy" = "Red")
ggplot(wealth_paths,aes(x=Day)) + geom_line(aes(y=Bond_Heavy, color='Bond_Heavy'),size=1.5) + geom_line(aes(y=Equal_Weight ,color='Equal_Weight'),size=1.5) + geom_line(aes(y=Foreign_Heavy,color='Foreign_Heavy'),size=1.5) + scale_y_continuous(labels = scales::dollar_format()) + labs(x='Day', y='Portfolio Paths', color='Legend', title = 'Worst Performing Periods By Portfolio')
# Getting the wealth paths for each portfolio for the worst profit.
minports =  data.frame(all_ending_wealths %>%  group_by(Type) %>% filter(profit == min(profit)))
wealth_paths = data.frame(t(minports %>% select(-Type,-profit)))
colnames(wealth_paths) = c('Bond_Heavy', 'Equal_Weight', 'Foreign_Heavy')
wealth_paths$Day = 1:20
colors = c("Bond_Heavy" = "Green", "Equal_Weight" = "Blue", "Foreign_Heavy" = "Red")
ggplot(wealth_paths,aes(x=Day)) + geom_line(aes(y=Bond_Heavy, color='Bond_Heavy'),size=1.5) + geom_line(aes(y=Equal_Weight ,color='Equal_Weight'),size=1.5) + geom_line(aes(y=Foreign_Heavy,color='Foreign_Heavy'),size=1.5) + scale_y_continuous(labels = scales::dollar_format()) + labs(x='Day', y='Portfolio Paths', color='Legend', title = 'Worst Performing Periods By Portfolio')  + scale_color_manual(values = colors)
View(welath_paths)
data.frame(all_ending_wealths %>%  group_by(Type) %>% filter(profit == min(profit)))
# Getting the wealth paths for each portfolio for the worst profit.
minports =  data.frame(all_ending_wealths %>%  group_by(Type) %>% filter(profit == min(profit)))
wealth_paths = data.frame(t(minports %>% select(-Type,-profit)))
colnames(wealth_paths) = c('Equal_Weight','Bond_Heavy', 'Foreign_Heavy')
wealth_paths$Day = 1:20
colors = c("Bond_Heavy" = "Green", "Equal_Weight" = "Blue", "Foreign_Heavy" = "Red")
ggplot(wealth_paths,aes(x=Day)) + geom_line(aes(y=Bond_Heavy, color='Bond_Heavy'),size=1.5) + geom_line(aes(y=Equal_Weight ,color='Equal_Weight'),size=1.5) + geom_line(aes(y=Foreign_Heavy,color='Foreign_Heavy'),size=1.5) + scale_y_continuous(labels = scales::dollar_format()) + labs(x='Day', y='Portfolio Paths', color='Legend', title = 'Worst Performing Periods By Portfolio')  + scale_color_manual(values = colors)
knitr::opts_chunk$set(echo = FALSE)
# output:
#   md_document:
#     variant: markdown_github
hist(greenbuildings$Rent, breaks = 50,xlab='Rent',main='Histogram of Rent')
knitr::opts_chunk$set(echo = FALSE)
# output:
#   md_document:
#     variant: markdown_github
# Load libraries
library(readr)
library(dplyr)
library(tidyverse)
library(ggcorrplot)
library(moments)
# Read in the data
greenbuildings <- read_csv("greenbuildings.csv")
greenbuildings = greenbuildings %>% select(-CS_PropertyID,-cluster) # Get rid of useless columns
# Convert categorical to factors and change binaries into 'Yes' and 'No'
data_plotting = greenbuildings
for (col_name in colnames(data_plotting)) {
data = data_plotting[[col_name]]
uniques = length(unique(data))
if ( uniques <=2) {
data = ifelse(data == 1,'Yes','No')
data_plotting[[col_name]] <- factor(data)
}
}
# We want to understand what's going on with those data points he's scrubbing.
# summary(greenbuildings)
low_occupany = greenbuildings %>% filter(leasing_rate < 10)
other = greenbuildings %>% filter(leasing_rate >= 10)
# None of the values seem too out of control to me. Just that they may be a bit older buildings.
# I believe it could be valid to remove these, because we don't really care much about
# Less than 10 seems pretty arbitrary though. So I'm leaning towards keeping it all.
# Might be that those buildings just get rented at weird times of the year or something.
# Either way, removing those data points or keeping them doesn't seem to change our conclusion much
# Only 215 data points
# summary(greenbuildings)
# summary(low_occupany)
# only getting rid of 1 green rating data point?
# sum(low_occupany$green_rating)
hist(greenbuildings$Rent, breaks = 50,xlab='Rent',main='Histogram of Rent')
rent_median = median(greenbuildings$Rent)
rent_mean = mean(greenbuildings$Rent)
rent_skew = skewness(greenbuildings$Rent)
abline(v=rent_median,lwd=2,col='blue')
abline(v=rent_mean,lwd=2,col='red')
text(100,1000, 'Mean = 28.42 \n Median = 25.16 \n Skew = 3.49')
ggplot(data_plotting,aes(x=cluster_rent,y=Rent)) + geom_point() + stat_smooth(method=lm,level=.95) + labs(title = 'Positive Association Between Rent of Cluster and Rent')
knitr::opts_chunk$set(echo = FALSE)
# output:
#   md_document:
#     variant: markdown_github
# Load libraries
library(readr)
library(dplyr)
library(tidyverse)
library(ggcorrplot)
library(moments)
# Read in the data
greenbuildings <- read_csv("greenbuildings.csv")
greenbuildings = greenbuildings %>% select(-CS_PropertyID,-cluster) # Get rid of useless columns
# Convert categorical to factors and change binaries into 'Yes' and 'No'
data_plotting = greenbuildings
for (col_name in colnames(data_plotting)) {
data = data_plotting[[col_name]]
uniques = length(unique(data))
if ( uniques <=2) {
data = ifelse(data == 1,'Yes','No')
data_plotting[[col_name]] <- factor(data)
}
}
# We want to understand what's going on with those data points he's scrubbing.
# summary(greenbuildings)
low_occupany = greenbuildings %>% filter(leasing_rate < 10)
other = greenbuildings %>% filter(leasing_rate >= 10)
# None of the values seem too out of control to me. Just that they may be a bit older buildings.
# I believe it could be valid to remove these, because we don't really care much about
# Less than 10 seems pretty arbitrary though. So I'm leaning towards keeping it all.
# Might be that those buildings just get rented at weird times of the year or something.
# Either way, removing those data points or keeping them doesn't seem to change our conclusion much
# Only 215 data points
# summary(greenbuildings)
# summary(low_occupany)
# only getting rid of 1 green rating data point?
# sum(low_occupany$green_rating)
hist(greenbuildings$Rent, breaks = 50,xlab='Rent',main='Histogram of Rent')
rent_median = median(greenbuildings$Rent)
rent_mean = mean(greenbuildings$Rent)
rent_skew = skewness(greenbuildings$Rent)
abline(v=rent_median,lwd=2,col='blue')
abline(v=rent_mean,lwd=2,col='red')
text(100,1000, 'Mean = 28.42 \n Median = 25.16 \n Skew = 3.49')
data_plotting %>% group_by(green_rating) %>%  summarise(Median_Rent = median(Rent)) %>% ggplot(aes(x=green_rating,y=Median_Rent,fill=green_rating)) +
geom_col() + geom_text(aes(label=Median_Rent),vjust=1.5,color='white') + labs(title = 'Median Rent by Greeness') + coord_cartesian(ylim=c(0,30)) + scale_y_continuous(breaks=seq(0,30,5))
data_no_nas = na.omit(greenbuildings) # remove na's from employment
# data_no_employ = greenbuildings %>% select(-empl_gr)
# ggcorrplot(cor(data_no_employ), hc.order = TRUE, outline.color = "white")
corry_matrix = cor(data_no_nas)
ggcorrplot(corry_matrix, hc.order = TRUE, outline.color = "white") + labs(title='Correlation Matrix')
ggplot(data_plotting) + geom_boxplot(aes(x=green_rating,y=age, fill=green_rating)) + labs(title = 'Age Distribution by Greenness')
med_age = median(data_plotting$age)
data_plotting <- data_plotting %>% mutate(younger = ifelse(age <= med_age,'Yes','No'))
# data_plotting$newer = factor(data_plotting$younger)
data_plotting %>% group_by(younger) %>%  summarise(Median_Rent = median(Rent)) %>% ggplot() +
geom_col(aes(x=younger,y=Median_Rent,fill=younger)) + labs(title = 'Median Rent by Age') + coord_cartesian(ylim=c(0,30)) + scale_y_continuous(breaks=seq(0,30,5))
data_plotting %>% group_by(green_rating,younger) %>%  summarise(Median_Rent = median(Rent), count = n()) %>% ggplot(aes(x=younger,y=Median_Rent,fill=green_rating)) + geom_col(position = 'dodge',color='black') + labs(title = 'Median Rent by Age & Greenness') + scale_y_continuous(breaks=seq(0,30,5))
green_only = data_plotting %>% filter(green_rating == "Yes")
ggplot(data=green_only, aes(x=class_a, fill=class_a)) + geom_bar(color='black') + labs(title = 'Count of Class A within Greenness')
data_plotting %>% group_by(green_rating,class_a) %>%  summarise(Median_Rent = median(Rent), count = n()) %>% ggplot(aes(x=class_a,y=Median_Rent, fill=green_rating)) + geom_col(position = 'dodge',color='black') + labs(title = 'Median Rent by Class A & Greenness')  + scale_y_continuous(breaks=seq(0,30,5))
ggplot(data_plotting,aes(x=cluster_rent,y=Rent)) + geom_point() + stat_smooth(method=lm,level=.95) + labs(title = 'Positive Association Between Rent of Cluster and Rent')
model_data = greenbuildings %>% select(-total_dd_07) # Remove total because it is just a combination of two other vars.
# model_data = greenbuildings %>% select(-cd_total_07,-hd_total07)
lm_model = lm(Rent~.,data=model_data)
summary(lm_model)
knitr::opts_chunk$set(echo = FALSE)
# output:
#   md_document:
#     variant: markdown_github
# Load libraries
set.seed(42)
library(readr)
library(dplyr)
library(tidyverse)
library(ggcorrplot)
library(moments)
library(mosaic)
library(quantmod)
library(foreach)
library(readr)
library(dplyr)
library(tidyverse)
library(ggcorrplot)
library(moments)
# Read in the data
greenbuildings <- read_csv("greenbuildings.csv")
greenbuildings = greenbuildings %>% select(-CS_PropertyID,-cluster) # Get rid of useless columns
# Convert categorical to factors and change binaries into 'Yes' and 'No'
data_plotting = greenbuildings
for (col_name in colnames(data_plotting)) {
data = data_plotting[[col_name]]
uniques = length(unique(data))
if ( uniques <=2) {
data = ifelse(data == 1,'Yes','No')
data_plotting[[col_name]] <- factor(data)
}
}
# We want to understand what's going on with those data points he's scrubbing.
# summary(greenbuildings)
low_occupany = greenbuildings %>% filter(leasing_rate < 10)
other = greenbuildings %>% filter(leasing_rate >= 10)
# None of the values seem too out of control to me. Just that they may be a bit older buildings.
# I believe it could be valid to remove these, because we don't really care much about
# Less than 10 seems pretty arbitrary though. So I'm leaning towards keeping it all.
# Might be that those buildings just get rented at weird times of the year or something.
# Either way, removing those data points or keeping them doesn't seem to change our conclusion much
# Only 215 data points
# summary(greenbuildings)
# summary(low_occupany)
# only getting rid of 1 green rating data point?
# sum(low_occupany$green_rating)
hist(greenbuildings$Rent, breaks = 50,xlab='Rent',main='Histogram of Rent')
rent_median = median(greenbuildings$Rent)
rent_mean = mean(greenbuildings$Rent)
rent_skew = skewness(greenbuildings$Rent)
abline(v=rent_median,lwd=2,col='blue')
abline(v=rent_mean,lwd=2,col='red')
text(100,1000, 'Mean = 28.42 \n Median = 25.16 \n Skew = 3.49')
data_plotting %>% group_by(green_rating) %>%  summarise(Median_Rent = median(Rent)) %>% ggplot(aes(x=green_rating,y=Median_Rent,fill=green_rating)) +
geom_col() + geom_text(aes(label=Median_Rent),vjust=1.5,color='white') + labs(title = 'Median Rent by Greeness') + coord_cartesian(ylim=c(0,30)) + scale_y_continuous(breaks=seq(0,30,5))
data_no_nas = na.omit(greenbuildings) # remove na's from employment
# data_no_employ = greenbuildings %>% select(-empl_gr)
# ggcorrplot(cor(data_no_employ), hc.order = TRUE, outline.color = "white")
corry_matrix = cor(data_no_nas)
ggcorrplot(corry_matrix, hc.order = TRUE, outline.color = "white") + labs(title='Correlation Matrix')
ggplot(data_plotting) + geom_boxplot(aes(x=green_rating,y=age, fill=green_rating)) + labs(title = 'Age Distribution by Greenness')
med_age = median(data_plotting$age)
data_plotting <- data_plotting %>% mutate(younger = ifelse(age <= med_age,'Yes','No'))
# data_plotting$newer = factor(data_plotting$younger)
data_plotting %>% group_by(younger) %>%  summarise(Median_Rent = median(Rent)) %>% ggplot() +
geom_col(aes(x=younger,y=Median_Rent,fill=younger)) + labs(title = 'Median Rent by Age') + coord_cartesian(ylim=c(0,30)) + scale_y_continuous(breaks=seq(0,30,5))
data_plotting %>% group_by(green_rating,younger) %>%  summarise(Median_Rent = median(Rent), count = n()) %>% ggplot(aes(x=younger,y=Median_Rent,fill=green_rating)) + geom_col(position = 'dodge',color='black') + labs(title = 'Median Rent by Age & Greenness') + scale_y_continuous(breaks=seq(0,30,5))
green_only = data_plotting %>% filter(green_rating == "Yes")
ggplot(data=green_only, aes(x=class_a, fill=class_a)) + geom_bar(color='black') + labs(title = 'Count of Class A within Greenness')
data_plotting %>% group_by(green_rating,class_a) %>%  summarise(Median_Rent = median(Rent), count = n()) %>% ggplot(aes(x=class_a,y=Median_Rent, fill=green_rating)) + geom_col(position = 'dodge',color='black') + labs(title = 'Median Rent by Class A & Greenness')  + scale_y_continuous(breaks=seq(0,30,5))
ggplot(data_plotting,aes(x=cluster_rent,y=Rent)) + geom_point() + stat_smooth(method=lm,level=.95) + labs(title = 'Positive Association Between Rent of Cluster and Rent')
model_data = greenbuildings %>% select(-total_dd_07) # Remove total because it is just a combination of two other vars.
# model_data = greenbuildings %>% select(-cd_total_07,-hd_total07)
lm_model = lm(Rent~.,data=model_data)
summary(lm_model)
# We want to understand what's going on with those data points he's scrubbing.
# summary(greenbuildings)
low_occupany = greenbuildings %>% filter(leasing_rate < 10)
boxplot(low_occupany$Rent)
View(low_occupany)
hist(low_occupany$leasing_rate)
200/7894
View(portfolio_weights)
View(wealth_paths)
# We want to understand what's going on with those data points he's scrubbing.
low_occupany = greenbuildings %>% filter(leasing_rate < 10)
other = greenbuildings %>% filter(leasing_rate >= 10)
# None of the values seem too out of control to me. Just that they may be a bit older buildings.
# I believe it could be valid to remove these, because we don't really care much about
# Less than 10 seems pretty arbitrary though. So I'm leaning towards keeping it all.
# Might be that those buildings just get rented at weird times of the year or something.
# Either way, removing those data points or keeping them doesn't seem to change our conclusion much
# Only 215 data points
summary(greenbuildings)
summary(low_occupany)
# only getting rid of 1 green rating data point?
# sum(low_occupany$green_rating)
# only getting rid of 1 green rating data point?
dim(low_occupany$green_rating)
# only getting rid of 1 green rating data point?
dim(low_occupany)
215/7894
# We want to understand what's going on with those data points he's scrubbing.
low_occupany = greenbuildings %>% filter(leasing_rate < 10)
other = greenbuildings %>% filter(leasing_rate >= 10)
# None of the values seem too out of control to me. Just that they may be a bit older buildings.
# I believe it could be valid to remove these, because we don't really care much about
# Less than 10 seems pretty arbitrary though. So I'm leaning towards keeping it all.
# Might be that those buildings just get rented at weird times of the year or something.
# Either way, removing those data points or keeping them doesn't seem to change our conclusion much
# Only 215 data points
# summary(greenbuildings)
# summary(low_occupany)
# dim(low_occupany)
knitr::opts_chunk$set(echo = FALSE)
# output:
#   md_document:
#     variant: markdown_github
# Load libraries
library(readr)
library(dplyr)
library(tidyverse)
library(ggcorrplot)
library(moments)
# Read in the data
greenbuildings <- read_csv("greenbuildings.csv")
greenbuildings = greenbuildings %>% select(-CS_PropertyID,-cluster) # Get rid of useless columns
# Convert categorical to factors and change binaries into 'Yes' and 'No'
data_plotting = greenbuildings
for (col_name in colnames(data_plotting)) {
data = data_plotting[[col_name]]
uniques = length(unique(data))
if ( uniques <=2) {
data = ifelse(data == 1,'Yes','No')
data_plotting[[col_name]] <- factor(data)
}
}
# We want to understand what's going on with those data points he's scrubbing.
low_occupany = greenbuildings %>% filter(leasing_rate < 10)
other = greenbuildings %>% filter(leasing_rate >= 10)
# None of the values seem too out of control to me. Just that they may be a bit older buildings.
# Less than 10 seems pretty arbitrary though. So I'm leaning towards keeping it all.
# Might be that those buildings just get rented at weird times of the year or something.
# Either way, removing those data points or keeping them doesn't seem to change our conclusion much
summary(greenbuildings)
summary(low_occupany)
dim(low_occupany)
hist(greenbuildings$Rent, breaks = 50,xlab='Rent',main='Histogram of Rent')
rent_median = median(greenbuildings$Rent)
rent_mean = mean(greenbuildings$Rent)
rent_skew = skewness(greenbuildings$Rent)
abline(v=rent_median,lwd=2,col='blue')
abline(v=rent_mean,lwd=2,col='red')
text(100,1000, 'Mean = 28.42 \n Median = 25.16 \n Skew = 3.49')
data_plotting %>% group_by(green_rating) %>%  summarise(Median_Rent = median(Rent)) %>% ggplot(aes(x=green_rating,y=Median_Rent,fill=green_rating)) +
geom_col() + geom_text(aes(label=Median_Rent),vjust=1.5,color='white') + labs(title = 'Median Rent by Greeness') + coord_cartesian(ylim=c(0,30)) + scale_y_continuous(breaks=seq(0,30,5))
data_no_nas = na.omit(greenbuildings) # remove na's from employment
# data_no_employ = greenbuildings %>% select(-empl_gr)
# ggcorrplot(cor(data_no_employ), hc.order = TRUE, outline.color = "white")
corry_matrix = cor(data_no_nas)
ggcorrplot(corry_matrix, hc.order = TRUE, outline.color = "white") + labs(title='Correlation Matrix')
ggplot(data_plotting) + geom_boxplot(aes(x=green_rating,y=age, fill=green_rating)) + labs(title = 'Age Distribution by Greenness')
med_age = median(data_plotting$age)
data_plotting <- data_plotting %>% mutate(younger = ifelse(age <= med_age,'Yes','No'))
# data_plotting$newer = factor(data_plotting$younger)
data_plotting %>% group_by(younger) %>%  summarise(Median_Rent = median(Rent)) %>% ggplot() +
geom_col(aes(x=younger,y=Median_Rent,fill=younger)) + labs(title = 'Median Rent by Age') + coord_cartesian(ylim=c(0,30)) + scale_y_continuous(breaks=seq(0,30,5))
data_plotting %>% group_by(green_rating,younger) %>%  summarise(Median_Rent = median(Rent), count = n()) %>% ggplot(aes(x=younger,y=Median_Rent,fill=green_rating)) + geom_col(position = 'dodge',color='black') + labs(title = 'Median Rent by Age & Greenness') + scale_y_continuous(breaks=seq(0,30,5))
green_only = data_plotting %>% filter(green_rating == "Yes")
ggplot(data=green_only, aes(x=class_a, fill=class_a)) + geom_bar(color='black') + labs(title = 'Count of Class A within Greenness')
data_plotting %>% group_by(green_rating,class_a) %>%  summarise(Median_Rent = median(Rent), count = n()) %>% ggplot(aes(x=class_a,y=Median_Rent, fill=green_rating)) + geom_col(position = 'dodge',color='black') + labs(title = 'Median Rent by Class A & Greenness')  + scale_y_continuous(breaks=seq(0,30,5))
knitr::opts_chunk$set(echo = FALSE)
# output:
#   md_document:
#     variant: markdown_github
# Load libraries
library(readr)
library(dplyr)
library(tidyverse)
library(ggcorrplot)
library(moments)
# Read in the data
greenbuildings <- read_csv("greenbuildings.csv")
greenbuildings = greenbuildings %>% select(-CS_PropertyID,-cluster) # Get rid of useless columns
# Convert categorical to factors and change binaries into 'Yes' and 'No'
data_plotting = greenbuildings
for (col_name in colnames(data_plotting)) {
data = data_plotting[[col_name]]
uniques = length(unique(data))
if ( uniques <=2) {
data = ifelse(data == 1,'Yes','No')
data_plotting[[col_name]] <- factor(data)
}
}
# We want to understand what's going on with those data points he's scrubbing.
low_occupany = greenbuildings %>% filter(leasing_rate < 10)
other = greenbuildings %>% filter(leasing_rate >= 10)
# None of the values seem too out of control to me. Just that they may be a bit older buildings.
# Less than 10 seems pretty arbitrary though. So I'm leaning towards keeping it all.
# Might be that those buildings just get rented at weird times of the year or something.
# Either way, removing those data points or keeping them doesn't seem to change our conclusion much
summary(greenbuildings)
summary(low_occupany)
dim(low_occupany)
hist(greenbuildings$Rent, breaks = 50,xlab='Rent',main='Histogram of Rent')
rent_median = median(greenbuildings$Rent)
rent_mean = mean(greenbuildings$Rent)
rent_skew = skewness(greenbuildings$Rent)
abline(v=rent_median,lwd=2,col='blue')
abline(v=rent_mean,lwd=2,col='red')
text(100,1000, 'Mean = 28.42 \n Median = 25.16 \n Skew = 3.49')
data_plotting %>% group_by(green_rating) %>%  summarise(Median_Rent = median(Rent)) %>% ggplot(aes(x=green_rating,y=Median_Rent,fill=green_rating)) +
geom_col() + geom_text(aes(label=Median_Rent),vjust=1.5,color='white') + labs(title = 'Median Rent by Greeness') + coord_cartesian(ylim=c(0,30)) + scale_y_continuous(breaks=seq(0,30,5))
data_no_nas = na.omit(greenbuildings) # remove na's from employment
# data_no_employ = greenbuildings %>% select(-empl_gr)
# ggcorrplot(cor(data_no_employ), hc.order = TRUE, outline.color = "white")
corry_matrix = cor(data_no_nas)
ggcorrplot(corry_matrix, hc.order = TRUE, outline.color = "white") + labs(title='Correlation Matrix')
ggplot(data_plotting) + geom_boxplot(aes(x=green_rating,y=age, fill=green_rating)) + labs(title = 'Age Distribution by Greenness')
med_age = median(data_plotting$age)
data_plotting <- data_plotting %>% mutate(younger = ifelse(age <= med_age,'Yes','No'))
# data_plotting$newer = factor(data_plotting$younger)
data_plotting %>% group_by(younger) %>%  summarise(Median_Rent = median(Rent)) %>% ggplot() +
geom_col(aes(x=younger,y=Median_Rent,fill=younger)) + labs(title = 'Median Rent by Age') + coord_cartesian(ylim=c(0,30)) + scale_y_continuous(breaks=seq(0,30,5))
data_plotting %>% group_by(green_rating,younger) %>%  summarise(Median_Rent = median(Rent), count = n()) %>% ggplot(aes(x=younger,y=Median_Rent,fill=green_rating)) + geom_col(position = 'dodge',color='black') + labs(title = 'Median Rent by Age & Greenness') + scale_y_continuous(breaks=seq(0,30,5))
green_only = data_plotting %>% filter(green_rating == "Yes")
ggplot(data=green_only, aes(x=class_a, fill=class_a)) + geom_bar(color='black') + labs(title = 'Count of Class A within Greenness')
data_plotting %>% group_by(green_rating,class_a) %>%  summarise(Median_Rent = median(Rent), count = n()) %>% ggplot(aes(x=class_a,y=Median_Rent, fill=green_rating)) + geom_col(position = 'dodge',color='black') + labs(title = 'Median Rent by Class A & Greenness')  + scale_y_continuous(breaks=seq(0,30,5))
data_plotting %>% group_by(LEED,Energystar)
data_plotting %>% group_by(LEED,Energystar) %>% summarise(Median_Rent = median(Rent))
new_fram = data_plotting %>% filter(LEED == 'Yes')
new_fram
new_fram = data_plotting %>% filter(LEED == 'Yes', Energystar == 'Yes')
new_fram
new_fram = data_plotting %>% filter(LEED == 'Yes'|Energystar == 'Yes')
new_fram
sum(greenbuildings$LEED)
sum(greenbuildings$Energystar)
model_data = greenbuildings %>% select(-total_dd_07) # Remove total because it is just a combination of two other vars.
# model_data = greenbuildings %>% select(-cd_total_07,-hd_total07)
lm_model = lm(Rent~.,data=model_data)
summary(lm_model)
setwd("C:/Users/User/OneDrive/Desktop/Machine Learning/Machine-Learning")
knitr::opts_chunk$set(echo = TRUE)
library(arules)
groceries = read.transactions("groceries.txt", format = "basket", sep = ",",
rm.duplicates = FALSE)
summary(groceries)
itemFrequencyPlot(groceries, topN = 10)
barplot(sort(itemFrequency(groceries), decreasing=FALSE)[0:10])
barplot(sort(itemFrequency(groceries), decreasing=FALSE)[0:10])
summary(itemFrequency(groceries)>0.01)
summary(itemFrequency(groceries)>0.001)
basket_model1 = apriori(groceries, parameter = list(support = 0.001, confidence = 0.50, target='rules'))
summary(basket_model1)
inspect(sort(basket_model1, by = 'lift')[1:10])
basket_model2 = apriori(groceries, parameter = list(support = 0.003, confidence = 0.50, target='rules', maxlen = 4))
summary(basket_model2)
inspect(sort(basket_model2, by = 'lift')[1:10])
knitr::opts_chunk$set(echo = FALSE)
library(arules)
groceries = read.transactions("groceries.txt", format = "basket", sep = ",",
rm.duplicates = FALSE)
summary(groceries)
itemFrequencyPlot(groceries, topN = 10)
barplot(sort(itemFrequency(groceries), decreasing=FALSE)[0:10])
barplot(sort(itemFrequency(groceries), decreasing=FALSE)[0:10])
summary(itemFrequency(groceries)>0.01)
summary(itemFrequency(groceries)>0.001)
basket_model1 = apriori(groceries, parameter = list(support = 0.001, confidence = 0.50, target='rules'))
summary(basket_model1)
inspect(sort(basket_model1, by = 'lift')[1:10])
basket_model2 = apriori(groceries, parameter = list(support = 0.003, confidence = 0.50, target='rules', maxlen = 4))
summary(basket_model2)
inspect(sort(basket_model2, by = 'lift')[1:10])
rm(list=ls())
library(tm)
library(tidyverse)
library(slam)
library(proxy)
library(magrittr)
library(dplyr)
library(ggplot2)
library(Rcpp)
train_folders=Sys.glob('ReutersC50/C50train/*')
for (x in train_folders) {
print(x)
# author_name=substring(x,first=55)
# article=Sys.glob(paste0(x,'/*.txt'))
# all_files=append(all_files,article)
# labels=append(labels,rep(author_name,length(article)))
}
?substri
substring
?substring
ReutersC50/C50train/AaronPressman"
'ReutersC50/C50train/AaronPressman"'
substring('haha',first=2)
#Getting all the file names in the sub-folders and making a vector with author names
all_files=NULL
labels=NULL
for (x in train_folders) {
author_name=substring(x,first=20)
print(author_name)
# article=Sys.glob(paste0(x,'/*.txt'))
# all_files=append(all_files,article)
# labels=append(labels,rep(author_name,length(article)))
}
for (x in train_folders) {
author_name=substring(x,first=21)
article=Sys.glob(paste0(x,'/*.txt'))
all_files=append(all_files,article)
labels=append(labels,rep(author_name,length(article)))
}
readerPlain = function(fname){
readPlain(elem=list(content=readLines(fname)),
id=fname, language='en') }
#reading all the sub-folders in the train folder
train_folders=Sys.glob('ReutersC50/C50train/*')
#Getting all the file names in the sub-folders and making a vector with author names
all_files=NULL
labels=NULL
for (x in train_folders) {
author_name=substring(x,first=21)
article=Sys.glob(paste0(x,'/*.txt'))
all_files=append(all_files,article)
labels=append(labels,rep(author_name,length(article)))
}
#Reading the files and removing .txt
files_combined = lapply(all_files, readerPlain)
names(files_combined) = all_files
#names(files_combined) = sub('.txt', '', names(files_combined))
#Creating a text-mining corpus
articles_raw = Corpus(VectorSource(files_combined))
View(articles_raw)
##Pre-processing and tokenization
documents = articles_raw %>%
tm_map(content_transformer(tolower))  %>%             # make everything lowercase
tm_map(content_transformer(removeNumbers)) %>%        # remove numbers
tm_map(content_transformer(removePunctuation)) %>%    # remove punctuation
tm_map(content_transformer(stripWhitespace))          # remove excess white-space
#remove stop words
documents = tm_map(documents, content_transformer(removeWords), stopwords("en"))
#Creating document-term matrix
DTM_train = DocumentTermMatrix(documents)
#reading all the sub-folders in the test folder
test_folders=Sys.glob('ReutersC50/C50test/*')
#Getting all the file names in the sub-folders and making a vector with author names
test_all_files=NULL
test_labels=NULL
for (x in test_folders) {
author_name=substring(x,first=20)
article=Sys.glob(paste0(x,'/*.txt'))
test_all_files=append(test_all_files,article)
test_labels=append(test_labels,rep(author_name,length(article)))
}
#Reading the files and removing .txt
test_files_combined = lapply(test_all_files, readerPlain)
names(test_files_combined) = test_all_files
#names(files_combined) = sub('.txt', '', names(files_combined))
#Creating a text-mining corpus
test_articles_raw = Corpus(VectorSource(test_files_combined))
test_articles_raw
?DocumentTermMatrix
install.packages('Rcpp')
install.packages("Rcpp")
